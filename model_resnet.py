# -*- coding: utf-8 -*-
"""model_resnet

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AA4QvA9xWl5R961oBxwcw4tANAYbUWdQ

## Устанавливаем и импортируем библиотеки
"""

pip install --ignore-installed PyYAML

!pip install transformers

!pip install -U augly[text]
!sudo apt-get install python3-magic

import re
import numpy as np
import pandas as pd
import yaml

import seaborn as sns
import matplotlib.pyplot as plt

import nlpaug.augmenter.word as naw
import random

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import matthews_corrcoef
from sklearn.metrics import f1_score, precision_score, recall_score

!pip install pytorch-pretrained-bert pytorch-nlp

import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from pytorch_pretrained_bert import BertTokenizer
from pytorch_pretrained_bert import BertForSequenceClassification
from torch.optim import AdamW
from transformers import AutoTokenizer

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/MyDrive/Colab_Notebooks/evolwe

"""## Подгружаю данные"""

with open(r'hello_nova_intents_0.2.2 (1).yaml') as file:
    df_dict = yaml.load(file, Loader=yaml.FullLoader)

my_df = pd.DataFrame(columns=['intents', 'examples'])

for i,v in enumerate(df_dict['data']):
  my_df = my_df.append({'examples': df_dict['data'][i]['examples'], 'intents': df_dict['data'][i]['intent']}, ignore_index=True)

my_df['intents'] = my_df['intents'].str.replace('\(\d\)', '')

df=my_df.explode('examples', ignore_index=True)

df

"""## Аугментация текста(т.к. небольшой размер данных и классы несбалансированны)"""

plt.figure(figsize=(10,6))
chart = sns.countplot(df['intents'])
plt.title("Number of texts per intent")
chart.set_xticklabels(chart.get_xticklabels(), rotation=90, horizontalalignment='right');

aug = naw.ContextualWordEmbsAug(model_path='bert-base-cased', action="insert", top_k=100)

list_uniq=list(df['intents'].unique())

for i in list_uniq:
  while df[df['intents'].isin([i])].value_counts().sum() != 100:
    df=df.append({'intents': i, 'examples':aug.augment(df[df['intents'].isin([i])]\
                                                       ['examples'].sample(n=1, random_state=random.randint(1,10)).iloc[0])}, ignore_index=True)
    if df.shape[0] % 5 == 0:
      df.to_csv('df_aug_top_k_100.csv', index=False)
    print(df.shape)

df.to_csv('df_aug_top_k_100.csv', index=False)

df['examples'] = df['examples'].apply(lambda x: re.sub(r"\s\’\s", r"'", x))
df['examples'] = df['examples'].apply(lambda x: re.sub(r"^\d+\s", r"", x))
df['examples'] = df['examples'].apply(lambda x: re.sub(r"\...", r"", x))
df['examples'] = df['examples'].apply(lambda x: re.sub(r"^\W", r"", x))
df['examples'] = df['examples'].apply(lambda x: re.sub(r"\s\W\s", r" ", x))
df['examples'] = df['examples'].apply(lambda x: re.sub(r"^\W+", r"", x))

df.to_csv('df_aug_top_k_100_fix.csv', index=False)

df = pd.read_csv('df_aug_top_k_100_fix.csv')
df

"""## Label encoder"""

le = LabelEncoder()
df['label'] = le.fit_transform(df['intents'])

df_train, df_test = train_test_split(df, test_size=0.1, random_state=666)

#распределение количества слов в примерах
plt.figure(figsize = (12,8))
sns.distplot(df_train.iloc[:,1].apply(lambda x: len(str(x).split())))
plt.show()

"""## Токенизируем текст с помощью готового BERT токенизатора"""

def preprocessing(df):
  max_len = 51

  tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
  inputs_list = []
  attention_masks_list = []
  inputs = []
  attention_masks = []
  
  ## Забираем паддинг, чтобы каждая последовательность была одной размерности + attention masks, чтобы на местах 0 не считались градиенты
  inputs_ = [tokenizer(x, padding='max_length', truncation=True) for x in df]
  for i in inputs_:
    inputs_list.append(i.get('input_ids'))
    attention_masks_list.append(i.get('attention_mask'))

  for i in inputs_list:
    inputs.append(i[:max_len])

  for i in attention_masks_list:
    attention_masks.append(i[:max_len])

  inputs = np.array(inputs)
  attention_masks = np.array(attention_masks)


  return inputs, attention_masks

inputs_test = preprocessing(df_test['examples'])[0]
attention_masks_test = preprocessing(df_test['examples'])[1]

train_x, valid_x, train_y, valid_y = train_test_split(preprocessing(df_train['examples'])[0],np.array(df_train['label']),
                                                            random_state=66, test_size=0.1)
train_masks, valid_masks, _, _ = train_test_split(preprocessing(df_train['examples'])[1], preprocessing(df_train['examples'])[0],
                                             random_state=66, test_size=0.1)

batch_size = 16

# конвертируем в тензор
train_x = torch.tensor(train_x)
valid_x = torch.tensor(valid_x)
train_y = torch.tensor(train_y)
valid_y = torch.tensor(valid_y)
train_masks = torch.tensor(train_masks)
valid_masks = torch.tensor(valid_masks)

train_data = TensorDataset(train_x, train_masks, train_y)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)
valid_data = TensorDataset(valid_x, valid_masks, valid_y)
valid_sampler = SequentialSampler(valid_data)
valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)

labels = np.array(df_test['label'])
prediction_inputs = torch.tensor(inputs_test)
prediction_masks = torch.tensor(attention_masks_test)
prediction_labels = torch.tensor(labels)
prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)
prediction_sampler = SequentialSampler(prediction_data)
prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)

# инициализируем модель
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=46)
model.to(device)

param_optimizer = list(model.named_parameters())
list_decay = ['bias', 'gamma', 'beta']
optimizer_grouped_parameters = [
    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in list_decay)],
     'weight_decay_rate': 0.01},
    {'params': [p for n, p in param_optimizer if any(nd in n for nd in list_decay)],
     'weight_decay_rate': 0.0}]

optimizer = AdamW(optimizer_grouped_parameters,
                     lr=2e-5)

# считаем точность модели
def accuracy(preds, labels):
    predict = np.argmax(preds, axis=1).flatten()
    labels_true = labels.flatten()
    N=len(labels_true)
    return np.sum(predict == labels_true) / N

train_loss_acc = []
epochs = 10

"""## Тренировка модели"""

for epoch in range(1, epochs+1):
  print(f'Epoch {epoch}/{epochs}')
  print('-' * 10)

  model.train()  
  train_loss = 0
  train_steps = 0

  for step, batch in enumerate(train_dataloader):

    batch = tuple(t.to(device) for t in batch)
    input_x, input_mask, labels_y = batch
    optimizer.zero_grad()

    loss = model(input_x, token_type_ids=None, attention_mask=input_mask, labels=labels_y)
    train_loss_acc.append(loss.item())    

    loss.backward()
    optimizer.step()

    train_loss += loss.item()
    train_steps += 1
  print("Train loss: {}".format(train_loss/train_steps))

  model.eval()

  eval_loss = 0
  eval_accuracy = 0
  eval_steps = 0

  for batch in valid_dataloader:
    batch = tuple(t.to(device) for t in batch)
    input_x, input_mask, labels_y = batch

    with torch.no_grad():
      logits = model(input_x, token_type_ids=None, attention_mask=input_mask)    

    logits = logits.detach().cpu().numpy()
    label_numpy = labels_y.to('cpu').numpy()

    def_eval_accuracy = accuracy(logits, label_numpy)    
    eval_accuracy += def_eval_accuracy
    eval_steps += 1
  print("Valid Accuracy: {}".format(eval_accuracy/eval_steps))

torch.save(model, 'model_v2.pt')

"""## TEST accuracy"""

test_accuracy = 0
test_steps = 0

model.eval()

pred = []
true_labels = []

for batch in prediction_dataloader:
  batch = tuple(t.to(device) for t in batch)
  input_x_test, input_mask_test, labels_y_test = batch

  with torch.no_grad():
    logits = model(input_x_test, token_type_ids=None, attention_mask=input_mask_test)

  logits = logits.detach().cpu().numpy()
  label_numpy_test = labels_y_test.to('cpu').numpy()  

  pred.append(logits)
  true_labels.append(label_numpy_test)

matthews_set = []
for i in range(len(true_labels)):
  matthews = matthews_corrcoef(true_labels[i],
                 np.argmax(pred[i], axis=1).flatten())
  matthews_set.append(matthews)
  
flatten_pred = [item for sublist in pred for item in sublist]
flatten_pred = np.argmax(flatten_pred, axis=1).flatten()
flatten_true_labels = [item for sublist in true_labels for item in sublist]

print(f'TEST ACCURACY: {matthews_corrcoef(flatten_true_labels, flatten_pred):.2f}')

print(f'TEST F1 score: {f1_score(flatten_true_labels,flatten_pred,average="macro"):.2f}')
print(f'TEST Precision score: {precision_score(flatten_true_labels,flatten_pred,average="macro"):.2f}')
print(f'TEST Recall score: {recall_score(flatten_true_labels,flatten_pred,average="macro"):.2f}')

